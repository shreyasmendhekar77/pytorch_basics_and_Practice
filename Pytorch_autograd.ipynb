{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4114c6d5",
   "metadata": {},
   "source": [
    "# Autograd - automatic gradient calcualtion module in pytorch "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd31a39",
   "metadata": {},
   "source": [
    "Gradient/derivative of the function with respect to x calculated using simple autograd function calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9b46683",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "753dacc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([6.0], requires_grad=True)\n",
    "y = x ** 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2294bcd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([12.])\n"
     ]
    }
   ],
   "source": [
    "y.backward()  # Compute the gradient\n",
    "print(x.grad)  # Print the gradient of x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee3e0e4",
   "metadata": {},
   "source": [
    "Now composite function and its gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf180c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=torch.tensor([4.0], requires_grad=True)\n",
    "y=x**2\n",
    "z=torch.sin(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1d32133",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6.], requires_grad=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "696b2aa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([36.], grad_fn=<PowBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18bc8fa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.9918], grad_fn=<SinBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe9da6f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-7.6613])\n"
     ]
    }
   ],
   "source": [
    "z.backward()  # Compute the gradient of z with respect to x\n",
    "print(x.grad)  # Print the gradient of x after the backward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4396da82",
   "metadata": {},
   "source": [
    "# Simple neural network using pytorch with autograd for gradient calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e87e300",
   "metadata": {},
   "source": [
    "Manual method for simple NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6070a858",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=torch.tensor([6.7], requires_grad=True)\n",
    "y=torch.tensor([0.0], requires_grad=True)\n",
    "\n",
    "# w- weights\n",
    "w=torch.tensor([1.0], requires_grad=True)\n",
    "\n",
    "# b- bias\n",
    "b=torch.tensor([0.0], requires_grad=True)\n",
    "\n",
    "z=w *x +b\n",
    "\n",
    "y_pred=torch.sigmoid(z)\n",
    "\n",
    "loss=(y_pred - y)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128cf970",
   "metadata": {},
   "source": [
    "Using Autograd for simple NN calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "79e3fdc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0082]) tensor([0.0012])\n"
     ]
    }
   ],
   "source": [
    "loss.backward()  # Compute the gradient of loss with respect to w and b\n",
    "print(w.grad, b.grad)  # Print the gradients of w and b after the backward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2dc1b23",
   "metadata": {},
   "source": [
    "# Applying autograd on the multiple values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9200a40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=torch.tensor([1.0,3.0,5.0], requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "75109344",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=(x**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fa78601e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.6667, 2.0000, 3.3333])\n"
     ]
    }
   ],
   "source": [
    "y.backward()  # Compute the gradient\n",
    "print(x.grad)  # Print the gradient of x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d6c6d5",
   "metadata": {},
   "source": [
    "# Clearing gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "59fd0b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=torch.tensor([2.0], requires_grad=True)\n",
    "y=x**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "09fa5172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.])\n"
     ]
    }
   ],
   "source": [
    "y.backward()  # Compute the gradient\n",
    "print(x.grad)  # Print the gradient of x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd8323b",
   "metadata": {},
   "source": [
    "Gradient accumalation ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b9b8764d",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[62]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# run again the backward pass\u001b[39;00m\n\u001b[32m      2\u001b[39m x.grad.zero_()  \u001b[38;5;66;03m# Clear the gradients\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43my\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Compute the gradient again\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(x.grad)  \u001b[38;5;66;03m# Print the gradient of x again\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Deep_Learning\\DL\\Lib\\site-packages\\torch\\_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Deep_Learning\\DL\\Lib\\site-packages\\torch\\autograd\\__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Deep_Learning\\DL\\Lib\\site-packages\\torch\\autograd\\graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mRuntimeError\u001b[39m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "# run again the backward pass\n",
    "x.grad.zero_()  # Clear the gradients\n",
    "y.backward()  # Compute the gradient again\n",
    "\n",
    "print(x.grad)  # Print the gradient of x again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8f9c83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ab558dac",
   "metadata": {},
   "source": [
    "# Stop the gradient calculation during the backward pass at time of prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "cb5027c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "y = x ** 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "69f30e6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3.], requires_grad=True)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5c2d30d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3.])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.requires_grad_(False)  # Stop tracking gradients for x\n",
    "y=x**2\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab14162",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3.])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "89ada980",
   "metadata": {},
   "source": [
    "detach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a2595dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=torch.tensor([6.7], requires_grad=True)\n",
    "x\n",
    "y=x**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "630e2d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "z=x.detach()  # Detach x from the computation graph\n",
    "print(z.requires_grad)  # Check if z requires gradients (should be False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8c16186c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6.7000])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a9846299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([13.4000])\n"
     ]
    }
   ],
   "source": [
    "y.backward()  # Compute the gradient\n",
    "print(x.grad)  # Print the gradient of x after detaching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7011a83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y1=z**2\n",
    "y1.backward()  # Compute the gradient of y1 with respect to z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699675c6",
   "metadata": {},
   "source": [
    "no grad function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f11434ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    y = x ** 2  # This will not track gradients\n",
    "    print(y.requires_grad)  # Should be False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c68993",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a66e10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
